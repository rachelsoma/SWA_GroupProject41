---
Title: Analysis of J.K. Rowling. Part B
output:
  html_document: default
  pdf_document: default
  word_document: default
---

Cover Sheet
========================================================


By including this statement, we, all the students listed in the table
below, declare that:

- We hold a copy of this assignment if the original is lost or damaged.

- We hereby certify that no part of this assignment has been copied from
  any other student's work or from any other source except where due
  acknowledgement is made in the assignment.

- No part of the assignment has been written for us by any other person
  except where collaboration has been authorised by the unit coordinator.

- We are aware that this work may be reproduced and submitted to plagiarism
  detection software programs for the purpose of detecting possible
  plagiarism; this software may retain a copy on its database for future
  plagiarism checking.

- We hereby certify that no part of this assignment or product has been
  submitted by any of us in another (previous or current) assessment, except
  where appropriately referenced, and with prior permission from the unit
  coordinator for this unit.

- We hereby certify that we have read and understand what the University
  considers to be academic misconduct, and that we are aware of thes
  penalties that may be imposed for academic misconduct.

Name                  | Student Number  | Contribution (%)
----------------------|-----------------|-----------------
Rachel Hardie         |  18820821       |   25%
Dylan Wang            |  18998014       |   25%
Dylan Yoo             |  18640377       |   25%
Sreenath Ramachandran |  18878716       |   25%

<div style="page-break-before:always;"></div>

```{r setup, message=FALSE, warning=FALSE}
library("tm")
library("rtweet")
library("twitteR")
library("igraph")
library("knitr")
tweets=read.csv("SWAProject.csv")
```

# 8.4 Examining the network of the tweets about J.K. Rowling


```{r DTM, results='hide', message=FALSE, warning=FALSE}
corpus=Corpus(VectorSource(tweets$text))
corpus = tm_map(corpus, function(x) iconv(x, to = 'UTF8', sub = 'byte'))
corpus = tm_map(corpus, function(x) iconv(x, to = 'ASCII', sub = ' '))
corpus = tm_map(corpus,removeNumbers)
corpus = tm_map(corpus, removeWords,c(stopwords(),"J.K. Rowling","https", "t.co"))
corpus = tm_map(corpus,removePunctuation)
corpus = tm_map(corpus,stripWhitespace)
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, removeWords,c(stopwords(),"jk","https", "t.co","rowl","jkrowl",
                                      "will","like","just","doe","far","quot","look",
                                      "take","make","jaqkcpmtg","ygorfremo","ask","peopl",
                                      "next","twitter","peopl","new","write","charact","got",
                                      "ohhgmljqrm","say","defend","respond", "vrmgsnrsa",
                                      "bbepgeik","nkuwxgsn","novrijqr","nlrbgguhxt"))
dtm = DocumentTermMatrix(corpus)
tweet.wdtm = weightTfIdf(dtm)
tweet.matrix = as.matrix(tweet.wdtm)
```

### 8.4.1 Finding the similarity index

To find the cosine similarity matrix we first find the normalised tweet matrix then use the TFIDF weighting obtained from frequencies in our document term matrix. 

```{r cosine}
#Find similarity matrix
S=tweet.matrix%*%t(tweet.matrix)
#to find the cosine similarity matrix we need  the normalised tweet matrix.
norm.tweet.matrix = diag(1/sqrt(rowSums(tweet.matrix^2))) %*% tweet.matrix
#cosine similarity
CS=norm.tweet.matrix%*%t(norm.tweet.matrix)

head(S)[1:20]
head(CS)[1:20]
```
####Difference between Cosine similarity and S
Cosine similarity gives the angle between the documents. The similarity matrix is more like the magnitude of the difference between the documents whereas the cosine similarity matrix gives the direction.Cosine similarity of the documents will be between 0 and 1. 

##8.4.2 histogram of retweet counts



```{r retweets}

retweetCount=tweets$retweet_count

hist(retweetCount,col="red",breaks = 50)

```

As the histogram of retweet count is exponenetial and skewed we take logarithm of retweet counts +1 so as to eliminate log 0 from the data

##8.4.3
```{r retweet_log}
retweet.log=log(retweetCount+1)

hist(retweet.log,col="blue",breaks = 50)
```

##8.4.4

To make the graph simpler we are taking the top 20 retweeted tweets.


```{r network_graphs}
graph=graph.adjacency(S,weighted = TRUE)
par(mai=c(0,0,0,0))
plot(simplify(graph),layout =layout.fruchterman.reingold, vertex.size=retweet.log*.9, vertex.label="",vertex.color=heat.colors(20),edge.width=0.5, edge.arrow.size=0)

id=order(retweet.log,decreasing = TRUE)[1:20]
tweets.imp=tweets[id,]
retweetCount.imp=tweets.imp$retweet_count
#retweetCount.imp
tweet.imp=tweet.matrix[id,]
s.imp=tweet.imp%*%t(tweet.imp)
set.seed(1500)
par(mai=c(0,0,0,0))
graph1=graph.adjacency(s.imp,weighted = TRUE)
plot(simplify(graph1),layout=layout.fruchterman.reingold,
     vertex.size=retweet.log*.9, edge.width=0.5,
     edge.arrow.size=0,vertex.label="",vertex.color=heat.colors(20))

```


###Results

Connected tweets mostly are the same tweets retweeted by different users.


##8.5.1 centrality measures 
###Degree centrality
```{r degree_centrality, warning=FALSE}

degCentrality=degree(graph)
topTweetsDegree=order(degCentrality,decreasing = TRUE)[1:10]
tweets$text[topTweetsDegree]

```

###Closeness centrality 

```{r closeness_centrality, warning=FALSE}
closenessCentrality=closeness(graph)
topTweetsCloseness=order(closeness(graph),decreasing = TRUE)[1:10]
tweets$text[topTweetsCloseness]
```

###Betweenness centrality 

```{r betweenness, warning=FALSE}
betweennessCentrality=betweenness(graph)
topTweetsBetweenness=order(betweenness(graph),decreasing = TRUE)[1:10]
tweets$text[topTweetsBetweenness]
```

##8.5.2 Page rank to calculate the most influential tweets
```{r page_rank, warning=FALSE}
adjacency.to.probability = function(A) {
  cols = ncol(A)
  for (a in 1:cols) {
    A[, a] = normalise(A[, a])
  }
  return(A)
}
normalise=function(x){
  if(sum(x)!=0){
    x=x/sum(x)
  }else{
    x=x
  }
  return(x)
}
T = adjacency.to.probability(S)
J = matrix(rep(1/1000, 1000 * 1000), 1000, 1000)
alpha=0.8
M = alpha * T + (1 - alpha) * J
M = adjacency.to.probability(M)
#Power method to find stationary distribution
#function to calculate Euclidean distance between two vectors.
differenceEuc=function(x,y){
  return(sqrt(sum((x-y)^2)))
}
stationary.distribution = function(T) {
  # first create the initial state distribution
  n = ncol(T)
  p = rep(0, n)
  p[1] = 1
  
  # now take a random walk until the state distribution reaches the
  # stationary distribution.
  p.old = rep(0, n)
  while (differenceEuc(p, p.old) > 1e-06) {
    p.old = p
    p = T %*% p.old
  }
  return(p)
}

p = stationary.distribution(M)
influentialTweets=order(p,decreasing = TRUE)[1:11]
#took 11 to get 10 distinct users as one user was repeat in top10.

tweets$text[influentialTweets]

```
##8.5.3 Influential Users
```{r influencers, warning=FALSE}
users=tweets$screen_name[influentialTweets]
unique(users)

```


##8.5.4 influence ratio
```{r influence_ratio, warning=FALSE}
InfluenceRatio= tweets$followers_count[influentialTweets]/
                tweets$friends_count[influentialTweets]

influence <- unique(data.frame(users,InfluenceRatio),descending=TRUE)
names(influence) <- c("User Names","Influence Ratio")
influence

#median followers in sample
followers.median=median(tweets$followers_count)
#median influence ratio in sample
influenceRatio.median=median(InfluenceRatio)
```

##8.5.5 Activity measures
```{r activities, warning=FALSE}
ActivityMeasure=tweets$statuses_count[influentialTweets]
activity <- unique(data.frame(users,ActivityMeasure))
names(activity) <- c("User Names","Activity Measure")
activity
```

##8.5.6 Scatter plot of users

```{r echo=FALSE}
plot(unique(InfluenceRatio),unique(ActivityMeasure),col=heat.colors(20),pch=20, text(InfluenceRatio,ActivityMeasure,labels = users,cex = .7,pos=3 ))+
abline(v=influenceRatio.median,col="blue")+title(main="Activity Measure and Influence Ratio of influential Tweet authors",sub="Median for all samples shown with blue line",cex.sub = 0.75,col.sub="blue")

```



##Findings
The most influential user 'culturalkumite' is a bot. The high influential ratio is due to it having 61 followers and following only 1 user while the median number of follows in the collected data is 461. Other users of influential tweets do not have a very high influence ratio and are most likely fans of  J.K.Rowling's work who follow more users rather than broadcasting without following, as appears the case with 'culturalkumite'.

###Conclusion
When we consider all of our findings together we can say that J.K Rowling is intertwined with her creations from the Harry Potter novels and the subsequent film adaptations. Many of the frequent terms are actors from the film adaptions and also the film "Fantastic Beasts and Where To Find Them: The Crimes of Grindewald" which is due to be released.

We also note that because J.K. Rowling is a very well known author and has a very active twitter account there can be a huge shift of conversation topics in a short period of time and our analysis is based on only 1000 tweets from the 28th September 2018.

Shared news articles and tweets about J.K. Rowling are likely to be retweeted heavily among clusters of users, particularly if a subset of twitter users consider a subject related to her find a particular announcement to be very good or very bad.

There were some users who were accusing Rowling of racism in the charecters she created, particularly for the 'Nagini' role in the film Fantastic Beasts. On the other side of this is general excitement at the idea of actor Macauley Culkin expressing a desire to be cast in the series.